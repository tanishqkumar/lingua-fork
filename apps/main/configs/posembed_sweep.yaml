# Positional Embedding Sweep Config
# Base config identical to default.yaml, but we'll vary pos_embed_type
#
# Sweep variants:
#   rope       - Rotary Position Embeddings (baseline)
#   nope       - No positional embedding
#   alibi      - Attention with Linear Biases
#   sinusoidal - Fixed sinusoidal (original Transformer)
#   learned    - Learned absolute position embeddings
#
# All other hyperparameters kept constant for fair comparison.

dump_base: null  # Auto-resolved by cluster detection
name: "posembed_sweep"
steps: 4000  # 1B tokens
seed: 42

optim:
    optimizer: adamw
    scheduler: cosine
    lr: 3e-3
    warmup: 100
    lr_min_ratio: 0.1
    clip: 1.0

distributed:
    fsdp_type: no_shard
    compile: true
    model_dtype: bf16

model:
    dim: 832
    n_layers: 4
    n_heads: 13  # head_dim = 64
    vocab_size: 100277  # cl100k_base
    norm_type: rmsnorm
    activation: silu
    pos_embed_type: rope  # Will be overridden per run

data:
    root_dir: null  # Auto-resolved by cluster detection
    sources:
        fineweb_edu_10bt_shuffled: 100.0
    batch_size: 32
    seq_len: 1024
    tokenizer:
        name: cl100k

checkpoint:
    dump:
        every: -1
    eval:
        every: -1

logging:
    freq: 50
    val_loss_every: 500
    val_loss_batches: 10
    wandb:
        project: "tanishqbot"
        entity: null
        tags: ["posembed_sweep", "34m"]
        group: "posembed_sweep"

eval: null
